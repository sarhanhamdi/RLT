#!/usr/bin/env python
# coding: utf-8

"""
Pipeline.py - Data Preparation Pipeline
Ce script pr√©pare tous les datasets (r√©gression et classification) 
et sauvegarde les fichiers CSV pr√©par√©s dans le dossier data_prepared.
"""

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
import os
from pathlib import Path
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer

# ============================================
# CONFIGURATION DES CHEMINS
# ============================================

# Chemin vers le dossier Data
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "Data")
OUTPUT_DIR = os.path.join(BASE_DIR, "data_prepared")

# Cr√©er le dossier de sortie s'il n'existe pas
os.makedirs(OUTPUT_DIR, exist_ok=True)


# ============================================
# LISTE DES DATASETS
# ============================================

REG_DATASETS = [
    "auto-mpg",
    "concrete_data",
    "HousingData",
    "ozone",
    "winequality-red",
    "winequality-white",
    "dataset_scenario1",
    "dataset_scenario2",
    "dataset_scenario3",
    "dataset_scenario4",
]

CLASS_DATASETS = [
    "BreastCanDT",
    "sonar",
    "parkinsons",
    "ReplicatedAcousticFeatures-ParkinsonDatabase"
]


# ============================================
# FONCTIONS UTILITAIRES
# ============================================

def normalize(name):
    """Normalise le nom d'un dataset pour la recherche de fichiers."""
    return name.lower().replace("_", "").replace("-", "").replace(" ", "")


def find_csv(name):
    """
    Trouve le fichier CSV correspondant au nom du dataset.
    """
    target = normalize(name)

    for f in os.listdir(DATA_DIR):
        if not f.lower().endswith(".csv"):
            continue

        base = normalize(f.replace(".csv", ""))

        if base == target:
            return f

        if target in base:
            return f

    raise FileNotFoundError(f"‚ùå CSV file for '{name}' not found in Data/")


def repair_csv_if_needed(path):
    """
    R√©pare les CSV malform√©s en d√©tectant automatiquement le s√©parateur.
    """
    try:
        df = pd.read_csv(path)
    except:
        df = pd.read_csv(path, engine="python")

    # Si le fichier a plus d'une colonne, c'est OK
    if df.shape[1] > 1:
        return df

    print(f"‚ö† Repairing malformed CSV: {os.path.basename(path)}")

    # Essayer le point-virgule
    try:
        df2 = pd.read_csv(path, sep=";")
        if df2.shape[1] > 1:
            df2.to_csv(path, index=False)
            return df2
    except:
        pass

    # Essayer la virgule
    try:
        df2 = pd.read_csv(path, sep=",")
        if df2.shape[1] > 1:
            df2.to_csv(path, index=False)
            return df2
    except:
        pass

    # Dernier recours : division manuelle
    df2 = df.iloc[:, 0].str.split("[;,]", expand=True)
    df2.columns = [f"col{i}" for i in range(df2.shape[1])]
    df2.to_csv(path, index=False)
    return df2


def fix_winequality(path: str) -> pd.DataFrame:
    """
    Charge et nettoie les datasets winequality (red & white).
    D√©tecte automatiquement le s√©parateur et convertit les colonnes.
    """
    # Essayer le point-virgule d'abord
    try:
        df = pd.read_csv(path, sep=";")
        if df.shape[1] > 1:
            df.columns = [c.strip().replace('"', "") for c in df.columns]
            df = df.apply(pd.to_numeric, errors="ignore")
            return df
    except Exception:
        pass

    # Essayer la virgule
    try:
        df = pd.read_csv(path, sep=",")
        if df.shape[1] > 1:
            df.columns = [c.strip().replace('"', "") for c in df.columns]
            df = df.apply(pd.to_numeric, errors="ignore")
            return df
    except Exception:
        pass

    # Si le fichier est malform√© (une seule colonne)
    raw = pd.read_csv(path, header=None)
    lines = raw.iloc[:, 0]

    header = lines.iloc[0].replace('"', "").split(";")
    data = [row.replace('"', "").split(";") for row in lines.iloc[1:]]

    df = pd.DataFrame(data, columns=header)

    # Convertir les colonnes num√©riques
    for col in df.columns:
        df[col] = pd.to_numeric(df[col], errors="ignore")

    return df


def auto_detect_target(df, dataset_name):
    """
    D√©tecte automatiquement la colonne cible pour chaque dataset.
    """
    name = dataset_name.lower()

    # BREAST CANCER
    if "breast" in name:
        return "diagnosis"

    # SONAR
    if "sonar" in name:
        return df.columns[-1]

    # PARKINSONS
    if "parkinson" in name:
        for col in df.columns:
            if col.lower() == "status":
                return col

    # Replicated Acoustic Parkinson Database
    if "replicatedacoustic" in name:
        for col in df.columns:
            if col.lower() == "status":
                return col

    # WINEQUALITY
    if "winequality" in name:
        return "quality"

    # AUTO MPG
    if "auto" in name and "mpg" in name:
        return "mpg"

    # CONCRETE
    if "concrete" in name:
        for c in df.columns:
            if "compressive" in c.lower():
                return c
            if "strength" in c.lower():
                return c

    # HOUSING DATASET (Boston)
    if "housing" in name:
        for col in df.columns:
            if col.lower() == "medv":
                return col
        raise ValueError("‚ùå HousingData target 'MEDV' not found in columns.")

    # OZONE
    if "ozone" in name:
        for col in df.columns:
            if "maxo3" in col.lower():
                return col
            if "ozone" in col.lower():
                return col

    # SCENARIOS
    if "scenario" in name:
        return "Y"

    raise ValueError(f"‚ùå Could not detect target for dataset '{dataset_name}'.")


# ============================================
# FONCTIONS DE PR√âPARATION
# ============================================

def prepare_regression(df, target, dataset_name):
    """
    Pr√©pare les donn√©es pour la r√©gression.
    Returns:
        X_skl : numpy array pour les mod√®les sklearn (scaled)
        y     : numpy array target
        X_rlt : numpy array pour RLT python (non-scaled)
        y_rlt : numpy array target pour RLT python
    """
    # Fix winequality datasets BEFORE anything
    if "winequality" in dataset_name.lower():
        data_dir = os.path.join(BASE_DIR, "Data")
        csv_path = os.path.join(data_dir, dataset_name + ".csv")
        df = fix_winequality(csv_path)

    # Supprimer les lignes o√π la cible est manquante
    df = df.dropna(subset=[target])

    # S√©parer les features et la cible
    y = df[target].astype(float).values
    X = df.drop(columns=[target])

    # Convertir les colonnes en num√©riques si possible
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors="coerce")

    # Supprimer les colonnes compl√®tement NaN
    X = X.dropna(axis=1, how="all")

    # Imputer les valeurs manquantes (median)
    imputer = SimpleImputer(strategy="median")
    X_imputed = imputer.fit_transform(X)

    # Scaler les features pour les mod√®les sklearn
    scaler = StandardScaler()
    X_skl = scaler.fit_transform(X_imputed)

    # Pr√©parer les donn√©es pour RLT (SANS scaling)
    X_rlt = X_imputed.copy()
    y_rlt = y.copy()

    return X_skl, y, X_rlt, y_rlt


def prepare_classification(df, target, dataset_name):
    """
    Pr√©pare les donn√©es pour la classification.
    Returns:
        X_skl : numpy array pour les mod√®les sklearn (scaled)
        y_skl : numpy array target (encoded)
        X_rlt : numpy array pour RLT python (non-scaled)
        y_rlt : numpy array target pour RLT python (encoded)
    """
    # Supprimer les lignes o√π la cible est manquante
    df = df.dropna(subset=[target])

    # S√©parer les features et la cible
    y = df[target]
    X = df.drop(columns=[target])

    # Convertir les colonnes en num√©riques si possible
    for col in X.columns:
        X[col] = pd.to_numeric(X[col], errors="coerce")

    # Supprimer les colonnes compl√®tement NaN
    X = X.dropna(axis=1, how="all")

    # Imputer les valeurs manquantes (median)
    imputer = SimpleImputer(strategy="median")
    X_imputed = imputer.fit_transform(X)

    # Encoder la cible si n√©cessaire (cat√©gorielle -> num√©rique)
    if y.dtype == 'object' or y.dtype.name == 'category':
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
    else:
        y_encoded = y.values.astype(int)

    # Scaler les features pour les mod√®les sklearn
    scaler = StandardScaler()
    X_skl = scaler.fit_transform(X_imputed)

    # Pr√©parer les donn√©es pour RLT (SANS scaling)
    X_rlt = X_imputed.copy()
    y_rlt = y_encoded.copy()

    return X_skl, y_encoded, X_rlt, y_rlt


# ============================================
# FONCTION PRINCIPALE DE PR√âPARATION
# ============================================

def prepare_all_datasets():
    """
    Pr√©pare tous les datasets (r√©gression et classification) 
    et sauvegarde les CSV dans data_prepared.
    """
    print("\n" + "="*60)
    print("  PR√âPARATION DES DATASETS")
    print("="*60 + "\n")

    all_errors = []

    # ============================================
    # PR√âPARATION DES DATASETS DE R√âGRESSION
    # ============================================
    print("\n" + "-"*60)
    print("  DATASETS DE R√âGRESSION")
    print("-"*60 + "\n")

    for name in REG_DATASETS:
        try:
            # G√©rer les sc√©narios (scenario1_p1000, scenario1_p200, etc.)
            if "scenario" in name.lower():
                # Pour les sc√©narios, on pr√©pare tous les fichiers correspondants
                # "dataset_scenario1" -> "scenario1"
                scenario_num = name.replace("dataset_", "").replace("dataset", "")
                # Normaliser pour trouver les fichiers (scenario1 -> trouve scenario1_p1000, etc.)
                scenario_normalized = normalize(scenario_num)
                matching_files = [f for f in os.listdir(DATA_DIR) 
                                if normalize(f.replace(".csv", "")).startswith(scenario_normalized) 
                                and f.endswith(".csv")]
                
                if not matching_files:
                    print(f"‚ö† Aucun fichier trouv√© pour {name}")
                    continue
                
                for csv_file in matching_files:
                    dataset_key = csv_file.replace(".csv", "")
                    full_path = os.path.join(DATA_DIR, csv_file)
                    
                    df = repair_csv_if_needed(full_path)
                    target = auto_detect_target(df, dataset_key)
                    
                    X_skl, y_skl, X_rlt, y_rlt = prepare_regression(df, target, dataset_key)
                    
                    # Cr√©er un DataFrame avec les features et la cible
                    df_out = pd.DataFrame(X_skl, columns=[f"feature_{i}" for i in range(X_skl.shape[1])])
                    df_out[target] = y_skl
                    
                    # Sauvegarder
                    out_path = os.path.join(OUTPUT_DIR, f"{dataset_key}_prepared.csv")
                    df_out.to_csv(out_path, index=False)
                    
                    print(f"‚úî Pr√©par√© : {dataset_key}")
                    print(f"   Shape: {df_out.shape}")
            else:
                # Pour les autres datasets
                csv_file = find_csv(name)
                full_path = os.path.join(DATA_DIR, csv_file)
                
                df = repair_csv_if_needed(full_path)
                target = auto_detect_target(df, name)
                
                X_skl, y_skl, X_rlt, y_rlt = prepare_regression(df, target, name)
                
                # Cr√©er un DataFrame avec les features et la cible
                df_out = pd.DataFrame(X_skl, columns=[f"feature_{i}" for i in range(X_skl.shape[1])])
                df_out[target] = y_skl
                
                # Sauvegarder
                out_path = os.path.join(OUTPUT_DIR, f"{name}_prepared.csv")
                df_out.to_csv(out_path, index=False)
                
                print(f"‚úî Pr√©par√© : {name}")
                print(f"   Shape: {df_out.shape}")

        except Exception as e:
            error_msg = f"‚ùå ERREUR lors de la pr√©paration de {name}: {e}"
            print(error_msg)
            all_errors.append(error_msg)

    # ============================================
    # PR√âPARATION DES DATASETS DE CLASSIFICATION
    # ============================================
    print("\n" + "-"*60)
    print("  DATASETS DE CLASSIFICATION")
    print("-"*60 + "\n")

    for name in CLASS_DATASETS:
        try:
            csv_file = find_csv(name)
            full_path = os.path.join(DATA_DIR, csv_file)
            
            df = repair_csv_if_needed(full_path)
            target = auto_detect_target(df, name)
            
            X_skl, y_skl, X_rlt, y_rlt = prepare_classification(df, target, name)
            
            # Cr√©er un DataFrame avec les features et la cible
            df_out = pd.DataFrame(X_skl, columns=[f"feature_{i}" for i in range(X_skl.shape[1])])
            df_out[target] = y_skl
            
            # Sauvegarder
            out_path = os.path.join(OUTPUT_DIR, f"{name}_prepared.csv")
            df_out.to_csv(out_path, index=False)
            
            print(f"‚úî Pr√©par√© : {name}")
            print(f"   Shape: {df_out.shape}")

        except Exception as e:
            error_msg = f"‚ùå ERREUR lors de la pr√©paration de {name}: {e}"
            print(error_msg)
            all_errors.append(error_msg)

    # ============================================
    # R√âSUM√â FINAL
    # ============================================
    print("\n" + "="*60)
    print("  R√âSUM√â")
    print("="*60 + "\n")
    
    if all_errors:
        print(f"‚ö† {len(all_errors)} erreur(s) rencontr√©e(s):")
        for err in all_errors:
            print(f"   {err}")
    else:
        print("‚úî Tous les datasets ont √©t√© pr√©par√©s avec succ√®s !")
    
    print(f"\nüìÅ Fichiers sauvegard√©s dans : {OUTPUT_DIR}")
    
    # Lister les fichiers cr√©√©s
    prepared_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith("_prepared.csv")]
    print(f"\nüìä Nombre de fichiers pr√©par√©s : {len(prepared_files)}")
    print("\nFichiers cr√©√©s :")
    for f in sorted(prepared_files):
        file_path = os.path.join(OUTPUT_DIR, f)
        file_size = os.path.getsize(file_path) / 1024  # Taille en KB
        print(f"   - {f} ({file_size:.2f} KB)")


# ============================================
# POINT D'ENTR√âE PRINCIPAL
# ============================================

if __name__ == "__main__":
    prepare_all_datasets()

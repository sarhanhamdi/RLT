{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b085aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, ExtraTreesClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44b53389",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "'auto-mpg',\n",
    "'breastCanDT',\n",
    "'concrete_data',\n",
    "'HousingData',\n",
    "'ozone',\n",
    "'parkinsons_clean',\n",
    "'sonar',\n",
    "'winequality-red',\n",
    "'winequality-white',\n",
    "]\n",
    "TARGET_COLS = {\n",
    "'auto-mpg': 'mpg', \n",
    "'breastCanDT': 'diagnosis', # e.g. 'diagnosis' or 'Class' or 'target'\n",
    "'concrete_data': 'concrete_compressive_strength',\n",
    "'Housing': 'MEDV', # Boston: 'MEDV' or 'target'\n",
    "'ozone': 'obs',\n",
    "'parkinsons_clean': 'status', # 1 or 0\n",
    "'sonar': 'R', \n",
    "'winequality-red': 'quality',\n",
    "'winequality-white': 'quality',\n",
    "}\n",
    "TASKS = {\n",
    "    'auto-mpg': 'regression',\n",
    "    'breastCancer': 'classification',\n",
    "    'concrete_data': 'regression',\n",
    "    'Housing': 'regression',\n",
    "    'ozone': 'regression',\n",
    "    'parkinsons_clean': 'classification',\n",
    "    'sonar': 'classification',\n",
    "    'winequality-red': 'regression',\n",
    "    'winequality-white': 'regression',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed678ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name):\n",
    "    df = pd.read_csv(f\"{name}.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f854427a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(DATASETS[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099b87f",
   "metadata": {},
   "source": [
    "### Detect feature columns and prepare pipline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fd741c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessor(X):\n",
    "    # Basic heuristic: numeric columns vs categorical (object or low-cardinality)\n",
    "    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "\n",
    "    # Also treat integer columns with low cardinality as categorical\n",
    "    for col in numeric_cols[:]:\n",
    "        if X[col].nunique() <= 10 and X[col].dtype in ['int64']:\n",
    "            numeric_cols.remove(col)\n",
    "            categorical_cols.append(col)\n",
    "\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),('scaler', StandardScaler())])\n",
    "    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n",
    "    preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_cols),('cat', categorical_transformer, categorical_cols)], remainder='drop')\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac89614",
   "metadata": {},
   "source": [
    "\n",
    "### Model runners :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bac5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(y_true, y_pred):\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return {'rmse': rmse, 'r2': r2}\n",
    "\n",
    "\n",
    "def evaluate_classification(y_true, y_pred, y_score=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary' if len(np.unique(y_true))==2 else 'macro')\n",
    "    auc = None\n",
    "    try:\n",
    "        if y_score is not None:\n",
    "            auc = roc_auc_score(y_true, y_score[:,1] if y_score.ndim==2 else y_score)\n",
    "    except Exception:\n",
    "        auc = None\n",
    "    return {'accuracy': acc, 'f1': f1, 'auc': auc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57742de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_dataset(name):\n",
    "    print(f\"\\n=== Dataset: {name} ===\")\n",
    "    df = load_dataset(name)\n",
    "    target_col = TARGET_COLS.get(name)\n",
    "    if target_col is None or target_col not in df.columns:\n",
    "        raise ValueError(f\"Target column for {name} not found in dataframe. Please update TARGET_COLS.\")\n",
    "\n",
    "    task = TASKS.get(name)\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # For classification, encode non-numeric labels\n",
    "    if task=='classification':\n",
    "        if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "            y = pd.factorize(y)[0]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    preprocessor = build_preprocessor(X_train)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # --- Random Forest ---\n",
    "    if task=='regression':\n",
    "        rf = Pipeline([('pre', preprocessor), ('rf', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1))])\n",
    "    else:\n",
    "        rf = Pipeline([('pre', preprocessor), ('rf', RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1))])\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    if task=='regression':\n",
    "        res = evaluate_regression(y_test, y_pred)\n",
    "    else:\n",
    "        y_score = rf.predict_proba(X_test) if hasattr(rf.named_steps['rf'], 'predict_proba') else None\n",
    "        res = evaluate_classification(y_test, y_pred, y_score)\n",
    "    res.update({'dataset': name, 'model': 'RandomForest'})\n",
    "    results.append(res)\n",
    "    print('RandomForest ->', res)\n",
    "\n",
    "    # --- Extra Trees ---\n",
    "    if task=='regression':\n",
    "        et = Pipeline([('pre', preprocessor), ('et', ExtraTreesRegressor(n_estimators=200, random_state=42, n_jobs=-1))])\n",
    "    else:\n",
    "        et = Pipeline([('pre', preprocessor), ('et', ExtraTreesClassifier(n_estimators=200, random_state=42, n_jobs=-1))])\n",
    "    et.fit(X_train, y_train)\n",
    "    y_pred = et.predict(X_test)\n",
    "    if task=='regression':\n",
    "        res = evaluate_regression(y_test, y_pred)\n",
    "    else:\n",
    "        y_score = et.predict_proba(X_test) if hasattr(et.named_steps['et'], 'predict_proba') else None\n",
    "        res = evaluate_classification(y_test, y_pred, y_score)\n",
    "    res.update({'dataset': name, 'model': 'ExtraTrees'})\n",
    "    results.append(res)\n",
    "    print('ExtraTrees ->', res)\n",
    "\n",
    "    # --- Gradient Boosting ---\n",
    "    if task=='regression':\n",
    "        gb = Pipeline([('pre', preprocessor), ('gb', GradientBoostingRegressor(n_estimators=200, random_state=42))])\n",
    "    else:\n",
    "        gb = Pipeline([('pre', preprocessor), ('gb', GradientBoostingClassifier(n_estimators=200, random_state=42))])\n",
    "    gb.fit(X_train, y_train)\n",
    "    y_pred = gb.predict(X_test)\n",
    "    if task=='regression':\n",
    "        res = evaluate_regression(y_test, y_pred)\n",
    "    else:\n",
    "        y_score = gb.predict_proba(X_test) if hasattr(gb.named_steps['gb'], 'predict_proba') else None\n",
    "        res = evaluate_classification(y_test, y_pred, y_score)\n",
    "    res.update({'dataset': name, 'model': 'GradientBoosting'})\n",
    "    results.append(res)\n",
    "    print('GradientBoosting ->', res)\n",
    "\n",
    "    # --- RF-log(p) : implemented for regression tasks as RF on log1p(target) ---\n",
    "    if task=='regression':\n",
    "        y_train_log = np.log1p(y_train)\n",
    "        rf_log = Pipeline([('pre', preprocessor), ('rf', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1))])\n",
    "        rf_log.fit(X_train, y_train_log)\n",
    "        y_pred_log = rf_log.predict(X_test)\n",
    "        y_pred = np.expm1(y_pred_log)\n",
    "        res = evaluate_regression(y_test, y_pred)\n",
    "        res.update({'dataset': name, 'model': 'RF-log1p'})\n",
    "        results.append(res)\n",
    "        print('RF-log1p ->', res)\n",
    "    else:\n",
    "        print('RF-log(p) skipped for classification (not applicable)')\n",
    "\n",
    "    # Save results\n",
    "    df_res = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "    return df_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75dbdca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset: auto-mpg ===\n",
      "RandomForest -> {'rmse': 1.997907639882282, 'r2': 0.9257597367596032, 'dataset': 'auto-mpg', 'model': 'RandomForest'}\n",
      "ExtraTrees -> {'rmse': 2.1469159253741155, 'r2': 0.9142727755613371, 'dataset': 'auto-mpg', 'model': 'ExtraTrees'}\n",
      "GradientBoosting -> {'rmse': 2.2314598469655293, 'r2': 0.9073880900181459, 'dataset': 'auto-mpg', 'model': 'GradientBoosting'}\n",
      "RF-log1p -> {'rmse': 1.9917091117837653, 'r2': 0.9262196844483978, 'dataset': 'auto-mpg', 'model': 'RF-log1p'}\n",
      "\n",
      "=== Dataset: breastCanDT ===\n",
      "Failed on breastCanDT: pos_label=1 is not a valid label. It should be one of ['B', 'M']\n",
      "\n",
      "=== Dataset: concrete_data ===\n",
      "RandomForest -> {'rmse': 5.528438940061759, 'r2': 0.8813877465051169, 'dataset': 'concrete_data', 'model': 'RandomForest'}\n",
      "ExtraTrees -> {'rmse': 5.256598386223905, 'r2': 0.8927656021998773, 'dataset': 'concrete_data', 'model': 'ExtraTrees'}\n",
      "GradientBoosting -> {'rmse': 4.934091245184691, 'r2': 0.9055202195137996, 'dataset': 'concrete_data', 'model': 'GradientBoosting'}\n",
      "RF-log1p -> {'rmse': 5.669009699832254, 'r2': 0.8752791900251481, 'dataset': 'concrete_data', 'model': 'RF-log1p'}\n",
      "\n",
      "=== Dataset: HousingData ===\n",
      "Failed on HousingData: Target column for HousingData not found in dataframe. Please update TARGET_COLS.\n",
      "\n",
      "=== Dataset: ozone ===\n",
      "Failed on ozone: Target column for ozone not found in dataframe. Please update TARGET_COLS.\n",
      "\n",
      "=== Dataset: parkinsons_clean ===\n",
      "Failed on parkinsons_clean: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.\n",
      "\n",
      "=== Dataset: sonar ===\n",
      "RandomForest -> {'accuracy': 0.8095238095238095, 'f1': 0.8518518518518519, 'auc': 0.9098765432098765, 'dataset': 'sonar', 'model': 'RandomForest'}\n",
      "ExtraTrees -> {'accuracy': 0.8333333333333334, 'f1': 0.8571428571428572, 'auc': 0.9296296296296296, 'dataset': 'sonar', 'model': 'ExtraTrees'}\n",
      "GradientBoosting -> {'accuracy': 0.8095238095238095, 'f1': 0.8461538461538461, 'auc': 0.8987654320987655, 'dataset': 'sonar', 'model': 'GradientBoosting'}\n",
      "RF-log(p) skipped for classification (not applicable)\n",
      "\n",
      "=== Dataset: winequality-red ===\n",
      "RandomForest -> {'rmse': 0.5534882281946022, 'r2': 0.5312219250138227, 'dataset': 'winequality-red', 'model': 'RandomForest'}\n",
      "ExtraTrees -> {'rmse': 0.5406958046119833, 'r2': 0.5526406551203694, 'dataset': 'winequality-red', 'model': 'ExtraTrees'}\n",
      "GradientBoosting -> {'rmse': 0.6013083907091219, 'r2': 0.44671990992859023, 'dataset': 'winequality-red', 'model': 'GradientBoosting'}\n",
      "RF-log1p -> {'rmse': 0.5595757297385452, 'r2': 0.5208535733529737, 'dataset': 'winequality-red', 'model': 'RF-log1p'}\n",
      "\n",
      "=== Dataset: winequality-white ===\n",
      "Failed on winequality-white: Target column for winequality-white not found in dataframe. Please update TARGET_COLS.\n",
      "        rmse        r2          dataset             model  accuracy        f1  \\\n",
      "0   1.997908  0.925760         auto-mpg      RandomForest       NaN       NaN   \n",
      "1   2.146916  0.914273         auto-mpg        ExtraTrees       NaN       NaN   \n",
      "2   2.231460  0.907388         auto-mpg  GradientBoosting       NaN       NaN   \n",
      "3   1.991709  0.926220         auto-mpg          RF-log1p       NaN       NaN   \n",
      "4   5.528439  0.881388    concrete_data      RandomForest       NaN       NaN   \n",
      "5   5.256598  0.892766    concrete_data        ExtraTrees       NaN       NaN   \n",
      "6   4.934091  0.905520    concrete_data  GradientBoosting       NaN       NaN   \n",
      "7   5.669010  0.875279    concrete_data          RF-log1p       NaN       NaN   \n",
      "8        NaN       NaN            sonar      RandomForest  0.809524  0.851852   \n",
      "9        NaN       NaN            sonar        ExtraTrees  0.833333  0.857143   \n",
      "10       NaN       NaN            sonar  GradientBoosting  0.809524  0.846154   \n",
      "11  0.553488  0.531222  winequality-red      RandomForest       NaN       NaN   \n",
      "12  0.540696  0.552641  winequality-red        ExtraTrees       NaN       NaN   \n",
      "13  0.601308  0.446720  winequality-red  GradientBoosting       NaN       NaN   \n",
      "14  0.559576  0.520854  winequality-red          RF-log1p       NaN       NaN   \n",
      "\n",
      "         auc  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n",
      "5        NaN  \n",
      "6        NaN  \n",
      "7        NaN  \n",
      "8   0.909877  \n",
      "9   0.929630  \n",
      "10  0.898765  \n",
      "11       NaN  \n",
      "12       NaN  \n",
      "13       NaN  \n",
      "14       NaN  \n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "for name in DATASETS:\n",
    "    try:\n",
    "        df_res = run_for_dataset(name)\n",
    "        all_results.append(df_res)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {name}: {e}\")\n",
    "\n",
    "\n",
    "if all_results:\n",
    "    concat = pd.concat(all_results, ignore_index=True)\n",
    "    res_concat = pd.DataFrame(concat)\n",
    "    print(res_concat)\n",
    "else:\n",
    "    print('No results to summarize.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b4816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

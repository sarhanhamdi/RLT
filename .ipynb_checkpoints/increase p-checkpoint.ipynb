{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e644b0a-0ece-4c4c-b041-3f8d20ea9168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers dans le dossier 'Data/' :\n",
      "==================================================\n",
      "1. .ipynb_checkpoints\n",
      "2. auto-mpg.csv\n",
      "3. BreastCanDT.csv\n",
      "4. concrete_data.csv\n",
      "5. dataset_scenario1.csv\n",
      "6. dataset_scenario2.csv\n",
      "7. dataset_scenario3.csv\n",
      "8. dataset_scenario4.csv\n",
      "9. HousingData.csv\n",
      "10. ozone.csv\n",
      "11. parkinsons.csv\n",
      "12. ReplicatedAcousticFeatures-ParkinsonDatabase.csv\n",
      "13. RLT_PROJECT.ipynb\n",
      "14. sonar.csv\n",
      "15. winequality-red.csv\n",
      "16. winequality-white.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# V√©rifier que le dossier data/ existe\n",
    "data_folder = 'Data'\n",
    "\n",
    "print(\"Fichiers dans le dossier 'Data/' :\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if os.path.exists(data_folder):\n",
    "    files = os.listdir(data_folder)\n",
    "    for i, file in enumerate(files, 1):\n",
    "        print(f\"{i}. {file}\")\n",
    "else:\n",
    "    print(\"Le dossier 'Data/' n'existe pas !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc8914d6-56fe-4eb5-b117-cf993ac303c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AUGMENTATION DES FEATURES √Ä 500\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìä HousingData\n",
      "======================================================================\n",
      "   Shape originale : (506, 14)\n",
      "   Valeurs manquantes detectees\n",
      "      Colonne 0 (numerique) : mediane = 0.25\n",
      "      Colonne 1 (numerique) : mediane = 0.00\n",
      "      Colonne 2 (numerique) : mediane = 9.69\n",
      "      Colonne 3 (binaire) : mode = 0.0\n",
      "      Colonne 6 (numerique) : mediane = 76.80\n",
      "      Colonne 12 (numerique) : mediane = 11.43\n",
      "   X : (506, 13), y : (506,)\n",
      "      Augmentation : 13 ‚Üí 500 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr.Console\\AppData\\Local\\Temp\\ipykernel_12152\\3059333420.py:154: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_df.iloc[:, col_idx].fillna(median_val, inplace=True)\n",
      "C:\\Users\\Dr.Console\\AppData\\Local\\Temp\\ipykernel_12152\\3059333420.py:149: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_df.iloc[:, col_idx].fillna(mode_val, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Sauvegard√© : datasets_augmented\\HousingData.csv\n",
      "\n",
      "======================================================================\n",
      "üìä BreastCanDT\n",
      "======================================================================\n",
      "   Shape originale : (569, 33)\n",
      "   Target encode : {0: 'B', 1: 'M'}\n",
      "   Valeurs manquantes detectees\n",
      "   ‚ùå ERREUR : 0\n",
      "\n",
      "======================================================================\n",
      "üìä parkinsons\n",
      "======================================================================\n",
      "   Shape originale : (195, 24)\n",
      "   X : (195, 22), y : (195,)\n",
      "      Augmentation : 22 ‚Üí 500 features\n",
      "   ‚úÖ Sauvegard√© : datasets_augmented\\parkinsons.csv\n",
      "\n",
      "======================================================================\n",
      "üìä sonar\n",
      "======================================================================\n",
      "   Shape originale : (207, 61)\n",
      "   Target encode : {0: 'M', 1: 'R'}\n",
      "   X : (207, 60), y : (207,)\n",
      "      Augmentation : 60 ‚Üí 500 features\n",
      "   ‚úÖ Sauvegard√© : datasets_augmented\\sonar.csv\n",
      "\n",
      "======================================================================\n",
      "üìä winequality-white\n",
      "======================================================================\n",
      "   Shape originale : (4898, 12)\n",
      "   X : (4898, 11), y : (4898,)\n",
      "      Augmentation : 11 ‚Üí 500 features\n",
      "   ‚úÖ Sauvegard√© : datasets_augmented\\winequality-white.csv\n",
      "\n",
      "======================================================================\n",
      "üìä winequality-red\n",
      "======================================================================\n",
      "   Shape originale : (1599, 12)\n",
      "   X : (1599, 11), y : (1599,)\n",
      "      Augmentation : 11 ‚Üí 500 features\n",
      "   ‚úÖ Sauvegard√© : datasets_augmented\\winequality-red.csv\n",
      "\n",
      "======================================================================\n",
      "üìä ReplicatedAcousticFeatures-ParkinsonDatabase\n",
      "======================================================================\n",
      "   Shape originale : (240, 48)\n",
      "   X : (240, 46), y : (240,)\n",
      "      Augmentation : 46 ‚Üí 500 features\n",
      "   ‚úÖ Sauvegard√© : datasets_augmented\\ReplicatedAcousticFeatures-ParkinsonDatabase.csv\n",
      "\n",
      "======================================================================\n",
      "üìä ozone\n",
      "======================================================================\n",
      "   Shape originale : (112, 14)\n",
      "   X : (112, 5), y : (112,)\n",
      "      Augmentation : 5 ‚Üí 500 features\n",
      "   ‚úÖ Sauvegard√© : datasets_augmented\\ozone.csv\n",
      "\n",
      "======================================================================\n",
      "üìä concrete_data\n",
      "======================================================================\n",
      "   Shape originale : (1030, 9)\n",
      "   X : (1030, 8), y : (1030,)\n",
      "      Augmentation : 8 ‚Üí 500 features\n",
      "   ‚úÖ Sauvegard√© : datasets_augmented\\concrete_data.csv\n",
      "\n",
      "======================================================================\n",
      "üìä auto_mpg\n",
      "======================================================================\n",
      "   Shape originale : (398, 9)\n",
      "   X : (398, 6), y : (398,)\n",
      "      Augmentation : 6 ‚Üí 500 features\n",
      "   ‚úÖ Sauvegard√© : datasets_augmented\\auto_mpg.csv\n",
      "\n",
      "======================================================================\n",
      "R√âSUM√â\n",
      "======================================================================\n",
      "\n",
      "‚úÖ R√©ussis : 9/10\n",
      "   HousingData                                        ‚Üí (506, 500)\n",
      "   parkinsons                                         ‚Üí (195, 500)\n",
      "   sonar                                              ‚Üí (207, 500)\n",
      "   winequality-white                                  ‚Üí (4898, 500)\n",
      "   winequality-red                                    ‚Üí (1599, 500)\n",
      "   ReplicatedAcousticFeatures-ParkinsonDatabase       ‚Üí (240, 500)\n",
      "   ozone                                              ‚Üí (112, 500)\n",
      "   concrete_data                                      ‚Üí (1030, 500)\n",
      "   auto_mpg                                           ‚Üí (398, 500)\n",
      "\n",
      "‚ùå √âchou√©s : 1\n",
      "   BreastCanDT : 0\n",
      "\n",
      "üìã Mappings des targets encod√©s : datasets_augmented\\target_mappings.json\n",
      "\n",
      "   Exemple de contenu :\n",
      "   ‚Ä¢ sonar: {0: 'M', 1: 'R'}\n",
      "\n",
      "üíæ Datasets sauvegard√©s dans : datasets_augmented/\n",
      "\n",
      "‚úÖ TERMIN√â !\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "============================================================================\n",
    "AUGMENTATION DES FEATURES √Ä 500\n",
    "============================================================================\n",
    "Ce script charge chaque dataset et augmente le nombre de features √† 500\n",
    "en cr√©ant des features synth√©tiques (signal + bruit, ratio 1:2)\n",
    "============================================================================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TARGET_P = 500  # Nombre final de features\n",
    "SEED = 42\n",
    "SIGNAL_NOISE_RATIO = 0.33  # Ratio 1:2 (1 part signal, 2 parts bruit)\n",
    "DATA_FOLDER = 'Data'\n",
    "OUTPUT_FOLDER = 'datasets_augmented'\n",
    "\n",
    "# Configuration des datasets\n",
    "DATASETS_CONFIG = [\n",
    "    {'name': 'HousingData', 'filepath': 'Data/HousingData.csv', 'target_col': 'MEDV', 'sep': ','},\n",
    "    {'name': 'BreastCanDT', 'filepath': 'Data/BreastCanDT.csv', 'target_col': 'diagnosis', 'sep': ','},\n",
    "    {'name': 'parkinsons', 'filepath': 'data/parkinsons.csv', 'target_col': 'status', 'sep': ','},\n",
    "    {'name': 'sonar', 'filepath': 'data/sonar.csv', 'target_col': 'R', 'sep': ','},\n",
    "    {'name': 'winequality-white', 'filepath': 'Data/winequality-white.csv', 'target_col': 'quality', 'sep': ';'},\n",
    "    {'name': 'winequality-red', 'filepath': 'Data/winequality-red.csv', 'target_col': 'quality', 'sep': ','},\n",
    "    {'name': 'ReplicatedAcousticFeatures-ParkinsonDatabase', 'filepath': 'Data/ReplicatedAcousticFeatures-ParkinsonDatabase.csv', 'target_col': 'Status', 'sep': ','},\n",
    "    {'name': 'ozone', 'filepath': 'Data/ozone.csv', 'target_col': 'maxO3', 'sep': ','},\n",
    "    {'name': 'concrete_data', 'filepath': 'Data/concrete_data.csv', 'target_col': 'concrete_compressive_strength', 'sep': ','},\n",
    "    {'name': 'auto_mpg', 'filepath': 'Data/auto-mpg.csv', 'target_col': 'mpg', 'sep': ','}\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# FONCTION : AUGMENTER FEATURES\n",
    "# ============================================================================\n",
    "\n",
    "def augment_features(X, target_p=TARGET_P, seed=SEED):\n",
    "    \"\"\"\n",
    "    Augmente le nombre de features de p_original √† target_p\n",
    "    \n",
    "    Pour chaque nouvelle feature :\n",
    "    - S√©lectionne une feature originale au hasard\n",
    "    - G√©n√®re du bruit al√©atoire\n",
    "    - Combine : nouvelle_feature = 1/3 * signal + 2/3 * bruit\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array (n_samples, p_original)\n",
    "    target_p : int (500)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_augmented : array (n_samples, 500)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n_samples, p_original = X.shape\n",
    "    n_new = target_p - p_original\n",
    "    \n",
    "    if n_new <= 0:\n",
    "        print(f\"      ‚ö†Ô∏è  D√©j√† {p_original} features\")\n",
    "        return X\n",
    "    \n",
    "    print(f\"      Augmentation : {p_original} ‚Üí {target_p} features\")\n",
    "    \n",
    "    # Cr√©er matrice pour nouvelles features\n",
    "    X_new = np.zeros((n_samples, n_new))\n",
    "    \n",
    "    for i in range(n_new):\n",
    "        # S√©lectionner une feature originale au hasard\n",
    "        idx = np.random.randint(0, p_original)\n",
    "        signal = X[:, idx]\n",
    "        \n",
    "        # G√©n√©rer bruit (m√™me √©chelle que le signal)\n",
    "        noise_scale = np.std(signal) if np.std(signal) > 0 else 1.0\n",
    "        noise = np.random.normal(0, noise_scale, n_samples)\n",
    "        \n",
    "        # Combiner (ratio 1:2)\n",
    "        X_new[:, i] = SIGNAL_NOISE_RATIO * signal + (1 - SIGNAL_NOISE_RATIO) * noise\n",
    "    \n",
    "    # Coller nouvelles features aux originales\n",
    "    X_augmented = np.hstack([X, X_new])\n",
    "    \n",
    "    return X_augmented\n",
    "\n",
    "# ============================================================================\n",
    "# FONCTION : TRAITER UN DATASET\n",
    "# ============================================================================\n",
    "\n",
    "def process_dataset(config):\n",
    "    \"\"\"\n",
    "    Charge un dataset et augmente ses features √† 500\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä {config['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Charger le CSV\n",
    "        filepath = config['filepath']\n",
    "        df = pd.read_csv(filepath, sep=config['sep'])\n",
    "        \n",
    "        print(f\"   Shape originale : {df.shape}\")\n",
    "        \n",
    "        # S√©parer X et y\n",
    "        target_col = config['target_col']\n",
    "        y = df[target_col]\n",
    "        X_df = df.drop(columns=[target_col])\n",
    "        \n",
    "        # Garder SEULEMENT les colonnes num√©riques pour X\n",
    "        X_df = X_df.select_dtypes(include=[np.number])\n",
    "        original_column_names = X_df.columns.tolist()\n",
    "        X = X_df.values\n",
    "        \n",
    "        # Encoder y si texte et sauvegarder le mapping\n",
    "        mapping = None\n",
    "        if y.dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            y_encoded = le.fit_transform(y)\n",
    "            # Cr√©er mapping : {0: \"B\", 1: \"M\", ...}\n",
    "            mapping = {int(i): str(label) for i, label in enumerate(le.classes_)}\n",
    "            print(f\"   Target encode : {mapping}\")\n",
    "            y = y_encoded\n",
    "        else:\n",
    "            y = y.values\n",
    "        \n",
    "        # Imputation intelligente des valeurs manquantes\n",
    "        if pd.isna(X).any():\n",
    "            print(f\"   Valeurs manquantes detectees\")\n",
    "            X_df = pd.DataFrame(X)\n",
    "            \n",
    "            for col_idx in range(X_df.shape[1]):\n",
    "                col = X_df.iloc[:, col_idx]\n",
    "    \n",
    "            if col.isna().any():\n",
    "                # V√©rifier si colonne binaire (seulement 0 et 1)\n",
    "                unique_vals = col.dropna().unique()\n",
    "                is_binary = set(unique_vals).issubset({0, 1, 0.0, 1.0})\n",
    "        \n",
    "            if is_binary:\n",
    "                # Imputation par mode (valeur la plus fr√©quente)\n",
    "                mode_val = col.mode()[0]\n",
    "                X_df.iloc[:, col_idx] = X_df.iloc[:, col_idx].fillna(mode_val)\n",
    "                print(f\"      Colonne {col_idx} (binaire) : mode = {mode_val}\")\n",
    "            else:\n",
    "                # Imputation par m√©diane pour colonnes num√©riques\n",
    "                median_val = col.median()\n",
    "                X_df.iloc[:, col_idx] = X_df.iloc[:, col_idx].fillna(median_val)\n",
    "                print(f\"      Colonne {col_idx} (numerique) : mediane = {median_val:.2f}\")\n",
    "\n",
    "            X = X_df.values\n",
    "\n",
    "        \n",
    "        X = X.astype(float)\n",
    "        y = y.astype(float)\n",
    "        \n",
    "        print(f\"   X : {X.shape}, y : {y.shape}\")\n",
    "        \n",
    "        # AUGMENTER LES FEATURES\n",
    "        X_augmented = augment_features(X, target_p=TARGET_P)\n",
    "        \n",
    "        # Sauvegarder\n",
    "        os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "        \n",
    "        # Cr√©er les noms de colonnes\n",
    "        n_original = len(original_column_names)\n",
    "        n_synthetic = X_augmented.shape[1] - n_original\n",
    "        \n",
    "        # Noms : [vraies features] + [synthetic_0, synthetic_1, ...]\n",
    "        column_names = original_column_names + [f'synthetic_{i}' for i in range(n_synthetic)]\n",
    "        \n",
    "        output_df = pd.DataFrame(X_augmented, columns=column_names)\n",
    "        output_df[target_col] = y\n",
    "        \n",
    "        output_path = os.path.join(OUTPUT_FOLDER, f\"{config['name']}.csv\")\n",
    "        output_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"   ‚úÖ Sauvegard√© : {output_path}\")\n",
    "        \n",
    "        return {\n",
    "            'name': config['name'], \n",
    "            'status': 'success', \n",
    "            'shape': X_augmented.shape,\n",
    "            'mapping': mapping  # Peut √™tre None si pas d'encoding\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ERREUR : {e}\")\n",
    "        return {\n",
    "            'name': config['name'], \n",
    "            'status': 'failed', \n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# EX√âCUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"AUGMENTATION DES FEATURES √Ä 500\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for config in DATASETS_CONFIG:\n",
    "        result = process_dataset(config)\n",
    "        results.append(result)\n",
    "    \n",
    "    # R√©sum√©\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"R√âSUM√â\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    success = [r for r in results if r['status'] == 'success']\n",
    "    failed = [r for r in results if r['status'] == 'failed']\n",
    "    \n",
    "    print(f\"\\n‚úÖ R√©ussis : {len(success)}/{len(results)}\")\n",
    "    \n",
    "    for r in success:\n",
    "        print(f\"   {r['name']:<50s} ‚Üí {r['shape']}\")\n",
    "    \n",
    "    if failed:\n",
    "        print(f\"\\n‚ùå √âchou√©s : {len(failed)}\")\n",
    "        for r in failed:\n",
    "            print(f\"   {r['name']} : {r['error']}\")\n",
    "    \n",
    "    # Sauvegarder les mappings dans un fichier JSON\n",
    "    mappings = {}\n",
    "    for r in success:\n",
    "        if r.get('mapping'):  # Si le dataset avait un target encod√©\n",
    "            mappings[r['name']] = r['mapping']\n",
    "    \n",
    "    if mappings:\n",
    "        import json\n",
    "        mapping_path = os.path.join(OUTPUT_FOLDER, 'target_mappings.json')\n",
    "        with open(mapping_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(mappings, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nüìã Mappings des targets encod√©s : {mapping_path}\")\n",
    "        print(\"\\n   Exemple de contenu :\")\n",
    "        for name, mapping in list(mappings.items())[:3]:\n",
    "            print(f\"   ‚Ä¢ {name}: {mapping}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Datasets sauvegard√©s dans : {OUTPUT_FOLDER}/\")\n",
    "    print(\"\\n‚úÖ TERMIN√â !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d8a179-f781-4c71-b144-1c53a421d33a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

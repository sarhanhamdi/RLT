{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1b2759-11e6-4d78-93f0-7ae17dc7d31f",
   "metadata": {},
   "source": [
    "## 1. Importation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1da962a9-c483-48cc-9336-12347e5c2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, ExtraTreesRegressor, ExtraTreesClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df418f0-06b1-41e6-9111-c4147cde2830",
   "metadata": {},
   "source": [
    "## 2. Load Data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f4da54eb-1cad-48a1-af96-3308454a52dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Le fichier {path} n'existe pas.\")\n",
    "\n",
    "    if not path.endswith(\".csv\"):\n",
    "        raise ValueError(\"Format non support√© : seuls les fichiers .csv sont accept√©s.\")\n",
    "\n",
    "    print(f\"Chargement du dataset : {path}\")\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25635649-3c69-42d3-b70a-ce90bc727be6",
   "metadata": {},
   "source": [
    "## 2. Data Understanding : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4222a5ca-34fb-458a-969e-d27e93c3b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_understanding(df, target_column=None):\n",
    "\n",
    "    print(\"\\nüîπ Shape:\", df.shape)\n",
    "\n",
    "    print(\"\\nüîπ First 5 rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\nüîπ Last 5 rows:\")\n",
    "    display(df.tail())\n",
    "\n",
    "    print(\"\\nüîπ Info:\")\n",
    "    print(df.info())\n",
    "\n",
    "    print(\"\\nüîπ Data types:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print(\"\\nüîπ Missing values per column:\")\n",
    "    missing_vals = df.isnull().sum()\n",
    "    display(missing_vals[missing_vals > 0])\n",
    "\n",
    "    print(\"\\nüîπ Percentage of missing values per column:\")\n",
    "    missing_percent = (df.isnull().mean() * 100).round(2)\n",
    "    display(missing_percent[missing_percent > 0])\n",
    "\n",
    "    print(\"\\nüîπ Duplicate rows count:\", df.duplicated().sum())\n",
    "    if df.duplicated().sum() > 0:\n",
    "        print(\"üîπ Duplicate rows:\")\n",
    "        display(df[df.duplicated(keep=False)])\n",
    "\n",
    "    print(\"\\nüîπ Target variable preview:\")\n",
    "    if target_column and target_column in df.columns:\n",
    "        display(df[[target_column]].head())\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Target column not found or not provided.\")\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # D√©tection des colonnes num√©riques (une seule fois)\n",
    "    # -------------------------------------------------\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "    print(\"\\nüîπ Numeric columns:\", list(numeric_cols))\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # D√©tection des outliers\n",
    "    # -------------------------------------------------\n",
    "    outlier_counts = {}\n",
    "\n",
    "    if len(numeric_cols) > 0:\n",
    "        for col in numeric_cols:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "            outlier_counts[col] = len(outliers)\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Aucun champ num√©rique ‚Üí impossible de d√©tecter les outliers.\")\n",
    "\n",
    "    print(\"\\nüîπ Number of outliers per numeric column:\")\n",
    "    display(outlier_counts)\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Histogrammes des variables num√©riques\n",
    "    # -------------------------------------------------\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(\"\\nüìä Distribution des variables num√©riques :\")\n",
    "        df[numeric_cols].hist(bins=30, figsize=(12, 8))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Aucun champ num√©rique ‚Üí pas d‚Äôhistogrammes.\")\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Heatmap de corr√©lation\n",
    "    # -------------------------------------------------\n",
    "    if len(numeric_cols) > 1:\n",
    "        corr = df[numeric_cols].corr().abs()\n",
    "        mask = corr < 0.5\n",
    "\n",
    "        plt.figure(figsize=(18, 14))\n",
    "        ax = sns.heatmap(\n",
    "            corr, mask=mask, cmap=\"coolwarm\", annot=False,\n",
    "            linewidths=0.5, \n",
    "            cbar_kws={'label': 'Force de corr√©lation'}\n",
    "        )\n",
    "\n",
    "        plt.title(\"Heatmap des corr√©lations (seulement |corr| > 0.5)\", fontsize=16)\n",
    "\n",
    "        # L√©gende explicative\n",
    "        plt.text(\n",
    "            x=0.02, y=1.12,\n",
    "            s=(\n",
    "                \"L√©gende des couleurs :\\n\"\n",
    "                \"Rouge fonc√© ‚Üí Corr√©lation tr√®s positive (‚âà 0.8 √† 1.0)\\n\"\n",
    "                \"Bleu fonc√© ‚Üí Corr√©lation tr√®s n√©gative (‚âà -0.8 √† -1.0)\\n\"\n",
    "                \"Blanc ‚Üí Corr√©lation faible (< 0.5) ou masqu√©e\"\n",
    "            ),\n",
    "            fontsize=12,\n",
    "            transform=ax.transAxes,\n",
    "            verticalalignment='top',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.4\", fc=\"white\", ec=\"black\", alpha=0.8)\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Pas assez de colonnes num√©riques pour une heatmap.\")\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Barplot des valeurs manquantes\n",
    "    # -------------------------------------------------\n",
    "    missing = df.isnull().sum()\n",
    "    missing = missing[missing > 0]\n",
    "\n",
    "    if len(missing) > 0:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        missing.sort_values().plot(kind='barh')\n",
    "        plt.title(\"Valeurs manquantes par colonne\")\n",
    "        plt.xlabel(\"Nombre de valeurs manquantes\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"\\nAucune valeur manquante.\")\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Boxplots pour visualiser les outliers\n",
    "    # -------------------------------------------------\n",
    "    if len(numeric_cols) > 0:\n",
    "        for col in numeric_cols:\n",
    "            if df[col].dropna().nunique() > 1:\n",
    "                plt.figure(figsize=(6, 3))\n",
    "                sns.boxplot(x=df[col])\n",
    "                plt.title(f\"Boxplot ‚Äì {col}\")\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(f\"Impossible de tracer un boxplot pour {col} (pas assez de valeurs).\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Aucun champ num√©rique ‚Üí pas de boxplots.\")\n",
    "\n",
    "\n",
    "    return outlier_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3aaa45-8b3b-4693-85f7-1c924241c638",
   "metadata": {},
   "source": [
    "## 3. Data Preperation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9eed9241-b366-454d-9207-fd6921f8daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_iqr(df, numeric_columns, factor=1.5):\n",
    "    df = df.copy()\n",
    "    for col in numeric_columns:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower = Q1 - factor * IQR\n",
    "        upper = Q3 + factor * IQR\n",
    "        \n",
    "        df[col] = np.where(df[col] < lower, lower, df[col])\n",
    "        df[col] = np.where(df[col] > upper, upper, df[col])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def data_preparation(df, target_column, apply_capping=False):\n",
    "    df = df.copy()\n",
    "    original_cols = df.columns.tolist()\n",
    "    original_shape = df.shape\n",
    "\n",
    "    # 1) Remove columns 100% NaN (except target)\n",
    "    cols_to_drop = [col for col in df.columns \n",
    "                    if col != target_column and df[col].isna().all()]\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # 2) Remove zero variance columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "    numeric_cols_no_target = [col for col in numeric_cols if col != target_column]\n",
    "\n",
    "    zero_var_cols = []\n",
    "    if numeric_cols_no_target:\n",
    "        selector = VarianceThreshold(threshold=0.0)\n",
    "        selector.fit(df[numeric_cols_no_target])\n",
    "        zero_var_cols = [col for col, keep in zip(numeric_cols_no_target,\n",
    "                                                  selector.get_support())\n",
    "                         if not keep]\n",
    "        df = df.drop(columns=zero_var_cols)\n",
    "\n",
    "    # Ensure target exists\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' was removed!\")\n",
    "\n",
    "    # 3) Separate target\n",
    "    y = df[target_column]\n",
    "    X = df.drop(columns=[target_column])\n",
    "\n",
    "    # Detect column types\n",
    "    numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "    # 4) Apply IQR capping\n",
    "    if apply_capping:\n",
    "        X = cap_iqr(X, numeric_cols)\n",
    "\n",
    "    # 5) Build pipeline\n",
    "    numeric_pipeline = Pipeline([(\"scaler\", StandardScaler())])\n",
    "    categorical_pipeline = Pipeline([(\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", numeric_pipeline, numeric_cols),\n",
    "        (\"cat\", categorical_pipeline, categorical_cols)\n",
    "    ])\n",
    "\n",
    "    X_prepared = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Output summary\n",
    "    print(\"\\n=== DATA PREPARATION SUMMARY ===\")\n",
    "    print(f\"Shape before preparation: {original_shape}\")\n",
    "    print(f\"Shape after preparation: {df.shape}\")\n",
    "    print(f\"Columns removed: {cols_to_drop + zero_var_cols}\")\n",
    "\n",
    "    print(\"\\nTarget preview:\")\n",
    "    display(y.head())\n",
    "\n",
    "    print(\"\\nX (before preprocessing):\")\n",
    "    display(X.head())\n",
    "\n",
    "    print(\"\\nX_prepared shape:\", X_prepared.shape)\n",
    "    print(\"\\nPreprocessor used:\")\n",
    "    print(preprocessor)\n",
    "\n",
    "    removed_cols = cols_to_drop + zero_var_cols\n",
    "\n",
    "    return X, X_prepared, y, preprocessor, removed_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc809d3e-223d-4d57-b5a9-fe8d027842af",
   "metadata": {},
   "source": [
    "## 4. Modeling :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac10f71c-3d4d-4543-abff-d26a6cc32a2d",
   "metadata": {},
   "source": [
    "### Detect feature columns and prepare pipline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7c2ed799-5b80-49db-a6dd-2243b6f90c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessor(X):\n",
    "    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    # treat low-cardinality int as categorical\n",
    "    for col in numeric_cols[:]:\n",
    "        if X[col].nunique() <= 10:\n",
    "            numeric_cols.remove(col)\n",
    "            categorical_cols.append(col)\n",
    "\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "    return preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "63cd41eb-317a-455e-b1e7-d45d4e6750ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_dataset(name, df_data):\n",
    "    print(f\"\\n=== Dataset: {name} ===\")\n",
    "\n",
    "    target_col = TARGET_COLS[name]\n",
    "    task = TASKS[name]\n",
    "\n",
    "    X = df_data.drop(columns=[target_col])\n",
    "    y = df_data[target_col]\n",
    "\n",
    "    # encode target for classification\n",
    "    if task == 'classification' and y.dtype == 'object':\n",
    "        y = pd.factorize(y)[0]\n",
    "\n",
    "    # split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # build preprocessing pipeline\n",
    "    preprocessor = build_preprocessor(X_train)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # ---- 1) RANDOM FOREST ----\n",
    "    model = RandomForestRegressor if task=='regression' else RandomForestClassifier\n",
    "    rf = Pipeline([\n",
    "        ('pre', preprocessor),\n",
    "        ('model', model(n_estimators=200, random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    y_score = rf.predict_proba(X_test) if hasattr(rf.named_steps['model'], \"predict_proba\") else None\n",
    "\n",
    "    res = evaluate_regression(y_test, y_pred) if task=='regression' \\\n",
    "          else evaluate_classification(y_test, y_pred, y_score)\n",
    "    res.update({'dataset': name, 'model': 'RandomForest'})\n",
    "    results.append(res)\n",
    "    print('RandomForest ->', res)\n",
    "\n",
    "    # ---- 2) EXTRA TREES ----\n",
    "    model = ExtraTreesRegressor if task=='regression' else ExtraTreesClassifier\n",
    "    et = Pipeline([\n",
    "        ('pre', preprocessor),\n",
    "        ('model', model(n_estimators=200, random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "    et.fit(X_train, y_train)\n",
    "    y_pred = et.predict(X_test)\n",
    "    y_score = et.predict_proba(X_test) if hasattr(et.named_steps['model'], \"predict_proba\") else None\n",
    "\n",
    "    res = evaluate_regression(y_test, y_pred) if task=='regression' \\\n",
    "          else evaluate_classification(y_test, y_pred, y_score)\n",
    "    res.update({'dataset': name, 'model': 'ExtraTrees'})\n",
    "    results.append(res)\n",
    "    print('ExtraTrees ->', res)\n",
    "\n",
    "    # ---- 3) GRADIENT BOOSTING ----\n",
    "    model = GradientBoostingRegressor if task=='regression' else GradientBoostingClassifier\n",
    "    gb = Pipeline([\n",
    "        ('pre', preprocessor),\n",
    "        ('model', model(n_estimators=200, random_state=42))\n",
    "    ])\n",
    "    gb.fit(X_train, y_train)\n",
    "    y_pred = gb.predict(X_test)\n",
    "    y_score = gb.predict_proba(X_test) if hasattr(gb.named_steps['model'], \"predict_proba\") else None\n",
    "\n",
    "    res = evaluate_regression(y_test, y_pred) if task=='regression' \\\n",
    "          else evaluate_classification(y_test, y_pred, y_score)\n",
    "    res.update({'dataset': name, 'model': 'GradientBoosting'})\n",
    "    results.append(res)\n",
    "    print('GradientBoosting ->', res)\n",
    "\n",
    "    # ---- 4) RF sur log1p(y) ‚Äî seulement pour r√©gression ----\n",
    "    if task == 'regression':\n",
    "        y_train_log = np.log1p(y_train)\n",
    "        rf_log = Pipeline([\n",
    "            ('pre', preprocessor),\n",
    "            ('model', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "        rf_log.fit(X_train, y_train_log)\n",
    "        y_pred = np.expm1(rf_log.predict(X_test))\n",
    "\n",
    "        res = evaluate_regression(y_test, y_pred)\n",
    "        res.update({'dataset': name, 'model': 'RF-log1p'})\n",
    "        results.append(res)\n",
    "        print('RF-log1p ->', res)\n",
    "\n",
    "    df_res = pd.DataFrame(results)\n",
    "    return df_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4cc4be-a854-48b7-8971-21e8cd71539e",
   "metadata": {},
   "source": [
    "## 5. Evaluation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "48c50b3b-3287-4023-84dc-867532a4f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(y_true, y_pred):\n",
    "    return {\n",
    "        'rmse': mean_squared_error(y_true, y_pred, squared=False),\n",
    "        'r2': r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_classification(y_true, y_pred, y_score=None):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary' if len(np.unique(y_true))==2 else 'macro')\n",
    "    \n",
    "    auc = None\n",
    "    try:\n",
    "        if y_score is not None:\n",
    "            auc = roc_auc_score(y_true, y_score[:,1] if y_score.ndim==2 else y_score)\n",
    "    except:\n",
    "        auc = None\n",
    "\n",
    "    return {'accuracy': acc, 'f1': f1, 'auc': auc}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466fdb31-c9ed-4126-bc67-8b71e96142a4",
   "metadata": {},
   "source": [
    "## 6. Deploiment : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfce170-a7cb-42a4-9a02-ad212b24d07a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
